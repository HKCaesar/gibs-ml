{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_curve, roc_auc_score, auc\n",
    "\n",
    "import utils\n",
    "import models.net as net\n",
    "import models.data_loader_pixels as data_loader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (30.0, 16.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the .txt metadata about dataset\n",
    "data_dir = 'data/4326'\n",
    "layer_name = 'VIIRS_SNPP_CorrectedReflectance_TrueColor'\n",
    "missing_data_name = 'VIIRS_SNPP_CorrectedReflectance_Missing_Data'\n",
    "label_split = os.path.join(data_dir, layer_name + \".txt\")\n",
    "\n",
    "# Directory containing params.json\n",
    "model_dir = 'experiments/base_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "    \n",
    "    X = None\n",
    "    outputs_prob = None\n",
    "    outputs_pred = None\n",
    "    labels = None\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    with tqdm_notebook(total=len(dataloader)) as t:\n",
    "        for data_batch, labels_batch in dataloader:\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                data_batch, labels_batch = data_batch.cuda(async=True), labels_batch.cuda(async=True)\n",
    "            # fetch the next evaluation batch\n",
    "            data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output\n",
    "            output_batch = model(data_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            output_pred_batch = np.argmax(output_batch, axis=1)\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # save the outputs and labels\n",
    "            if X is None:\n",
    "                X = data_batch\n",
    "                outputs_prob = output_batch\n",
    "                outputs_pred = output_pred_batch\n",
    "                labels = labels_batch\n",
    "            else:\n",
    "                X = np.append(X, data_batch, axis=0)\n",
    "                outputs_prob = np.append(outputs_prob, output_batch, axis=0)\n",
    "                outputs_pred = np.append(outputs_pred, output_pred_batch, axis=0)\n",
    "                labels = np.append(labels, labels_batch, axis=0)\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data[0]\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Eval metrics : \" + metrics_string)\n",
    "    \n",
    "    return X, outputs_prob, outputs_pred, labels, metrics_mean\n",
    "\n",
    "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm_notebook(total=len(dataloader)) as t:\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(async=True), labels_batch.cuda(async=True)\n",
    "            # convert to torch Variables\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output and loss\n",
    "            output_batch = model(train_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                                 for metric in metrics}\n",
    "                summary_batch['loss'] = loss.data[0]\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.data[0])\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics, params, model_dir,\n",
    "                       restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(model_dir, restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    best_val_score = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        _, _, _, _, val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)\n",
    "\n",
    "        val_score = val_metrics['AUC']\n",
    "        is_best = val_score>best_val_score\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            print(\"best_val_score {:05.3f}\".format(best_val_score))\n",
    "            print(\"val_score {:05.3f}\".format(val_score))\n",
    "            best_val_score = val_score\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Hyperparameters from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "print(\"GPU available: {}\".format(params.cuda))\n",
    "\n",
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and optimizer\n",
    "model = net.Net(params, num_classes=8192*4096).cuda() if params.cuda else net.Net(params, num_classes=8192*4096)\n",
    "\n",
    "# Define the loss\n",
    "loss_fn = net.loss_fn\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch metrics\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x num_pixels - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "def recall(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the recall, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) recall in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return recall_score(labels, outputs)\n",
    "\n",
    "def precision(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the precision, given the outputs and labels for all images. \n",
    "    \n",
    "    The precision is the ratio tp / (tp + fp) where tp is the number \n",
    "    of true positives and fp the number of false positives. \n",
    "    \n",
    "    The precision is intuitively the ability of the classifier not to \n",
    "    label as positive a sample that is negative.\n",
    "    \n",
    "    The best value is 1 and the worst value is 0.\n",
    "    \n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) precision in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return precision_score(labels, outputs)\n",
    "\n",
    "def f1_metric(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the F1 score, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) F1 score in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return f1_score(labels, outputs)\n",
    "\n",
    "def roc_graph(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute Receiver operating characteristic (ROC).\n",
    "    \n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "        \n",
    "    Returns: fpr, tpr, thresholds\n",
    "    \"\"\"\n",
    "    outputs = outputs[:,1]\n",
    "    return roc_curve(labels, outputs)\n",
    "\n",
    "def roc_auc_metric(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "    \"\"\"\n",
    "    outputs = outputs[:,1]\n",
    "    return roc_auc_score(labels, outputs)\n",
    "\n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'AUC': roc_auc_metric, # area under receiver operator characteristic curve\n",
    "    'F1': f1_metric, # weighted average of precision and recall\n",
    "    'recall': recall, \n",
    "    'precision': precision,\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# borrowed from http://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "# and http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# define a training image loader that specifies transforms on images. See documentation for more details.\n",
    "train_transformer = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
    "    # transforms.RandomVerticalFlip(), # randomly flip image vertically\n",
    "    transforms.ToTensor(), # transform it into a torch tensor\n",
    "    ])  \n",
    "\n",
    "# loader for evaluation, no data augmentation (e.g. horizontal flip)\n",
    "eval_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(), # transform it into a torch tensor\n",
    "    ])  \n",
    "\n",
    "# fetch dataloaders\n",
    "train_dl = data_loader.fetch_dataloader('train', train_transformer, data_dir, layer_name, missing_data_name, label_split, params)\n",
    "val_dl = data_loader.fetch_dataloader('val', eval_transformer, data_dir, layer_name, missing_data_name, label_split, params)\n",
    "test_dl = data_loader.fetch_dataloader('test', eval_transformer, data_dir, layer_name, missing_data_name, label_split, params)\n",
    "\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload weights from the saved file\n",
    "# Name of the file in --model_dir containing weights to reload before training\n",
    "restore_file = None # 'best' or 'last' or None\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_dl, val_dl, optimizer, loss_fn, metrics, params, model_dir, restore_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'evaluate.log'))\n",
    "\n",
    "logging.info(\"Starting evaluation\")\n",
    "\n",
    "# Reload weights from the saved file\n",
    "# Name of the file in --model_dir containing weights to reload before evaluation\n",
    "restore_eval_file = None # 'best' or 'last' or None\n",
    "map_location = None if params.cuda else 'cpu'\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_eval_file + '.pth.tar'), model, map_location=map_location)\n",
    "\n",
    "# Evaluate\n",
    "X_test, y_test_prob, y_test_pred, y_test, test_metrics = evaluate(model, loss_fn, test_dl, metrics, params)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_eval_file))\n",
    "utils.save_dict_to_json(test_metrics, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fpr, test_tpr, test_thresholds = roc_graph(y_test_prob, y_test)\n",
    "test_roc_auc = roc_auc_metric(y_test_prob, y_test)\n",
    "print('ROC curve (area = %0.3f)' % test_roc_auc)\n",
    "# test_roc_auc = test_metrics['AUC']\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(test_fpr, test_tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % test_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for test set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(results, categories, normalizeMatrix = True, plotConfusion=False):\n",
    "    \"\"\" Generate confusion matrix\n",
    "    - results = list of tuples of (correct label, predicted label)\n",
    "    - categories = list of category names\n",
    "    Returns confusion matrix; rows are correct labels and columns are predictions\n",
    "    \"\"\"\n",
    "    # Empty confusion matrix\n",
    "    matrix = np.zeros((len(categories),len(categories)))\n",
    "\n",
    "    # Iterate over all labels and populate matrix\n",
    "    for label, pred in results:\n",
    "        matrix[label, pred] += 1\n",
    "\n",
    "    # Print matrix and percent accuracy\n",
    "    accuracy = float(np.trace(matrix)) * 100.0 / len(results)\n",
    "    print('Accuracy: ' +  str(accuracy) + '%')\n",
    "\n",
    "    print(\"Non-normalized conf_matrix:\")\n",
    "    print(matrix)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    if normalizeMatrix:\n",
    "      s = np.sum(matrix,1) # Sum each row\n",
    "      for i in range(matrix.shape[0]):\n",
    "        # Normalization handles class imbalance in training set\n",
    "        matrix[i,:] /= s[i]\n",
    "\n",
    "    # Save matrix to file:\n",
    "    # np.save(\"confusion_matrix.npy\",matrix)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    if plotConfusion:\n",
    "        plot_confusion_matrix(matrix, categories)\n",
    "\n",
    "def plot_confusion_matrix(cm, names=None, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(4)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add labels to confusion matrix:\n",
    "    if names is None:\n",
    "        names = range(cm.shape[0])\n",
    "\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Correct label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (30.0, 16.0) # set default size of plots\n",
    "\n",
    "classes = ['normal', 'abnormal']\n",
    "confusion_matrix(list(zip(y_test_pred, y_test)), classes, plotConfusion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (30.0, 16.0) # set default size of plots\n",
    "\n",
    "# Visualize misclassified examples from the validation dataset.\n",
    "num_classes = len(classes)\n",
    "max_examples = 5\n",
    "\n",
    "incorrect_idx = [idx for idx, incorrect in enumerate(np.array(y_test) != np.array(y_test_pred)) if incorrect]\n",
    "if len(incorrect_idx) > max_examples:\n",
    "    incorrect_idx = incorrect_idx[:max_examples]\n",
    "num_incorrect = len(incorrect_idx)\n",
    "\n",
    "for y, idx in enumerate(incorrect_idx):\n",
    "    for i in range(3):\n",
    "        plt_idx = i * num_incorrect + y + 1\n",
    "        if i == 0:\n",
    "            plt.subplot(3, num_incorrect, plt_idx)\n",
    "            plt.imshow(np.squeeze(X_test[idx]).astype('uint8'))\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Prediction: %s\\n Truth: %s\" % (classes[y_test_pred[idx]], classes[y_test[idx]]))\n",
    "        elif i == 1:\n",
    "            plt.subplot(3, num_incorrect, plt_idx)\n",
    "            log_probabilities = y_test_prob[idx]\n",
    "            probabilities = np.exp(log_probabilities)\n",
    "            y_pos = np.arange(len(classes))\n",
    "            plt.bar(y_pos, probabilities)\n",
    "            plt.xticks(y_pos, classes, rotation=45)\n",
    "            plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(classes)\n",
    "# img_size = (16, 8)\n",
    "# num_channels = 1\n",
    "\n",
    "# # Visualize the learned weights for each class.\n",
    "# # Depending on your choice of learning rate and regularization strength, these may\n",
    "# # or may not be nice to look at.\n",
    "# w = model.conv3.weight.data.numpy() # convert to numpy arrays\n",
    "# print(w.shape)\n",
    "# w = w.reshape(num_classes, img_size[1], img_size[0], num_channels)\n",
    "# w_min, w_max = np.min(w), np.max(w)\n",
    "# for i in range(num_classes):\n",
    "#     plt.subplot(2, 5, i + 1)\n",
    "      \n",
    "#     # Rescale the weights to be between 0 and 255\n",
    "#     wimg = 255.0 * (w[i, :, :, :].squeeze() - w_min) / (w_max - w_min)\n",
    "#     plt.imshow(wimg.astype('uint8'))\n",
    "#     plt.axis('off')\n",
    "#     plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on New Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the .txt metadata about dataset\n",
    "data_dir = 'data/4326'\n",
    "layer_name = 'VIIRS_SNPP_CorrectedReflectance_TrueColor'\n",
    "new_test_imgs = os.path.join(data_dir, \"NEW_\" + layer_name + \".txt\")\n",
    "\n",
    "# Directory containing params.json\n",
    "model_dir = 'experiments/base_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# Set the model input size\n",
    "IMG_DIM = (128, 256)\n",
    "IMG_PADDING = (0, 64, 0, 64) # left, top, right, bottom borders\n",
    "IMG_BINZARIZE = True # use black and white image\n",
    "\n",
    "# loader for evaluation, no data augmentation (e.g. horizontal flip)\n",
    "eval_transformer = transforms.Compose([\n",
    "    transforms.Resize(IMG_DIM),  # resize the image\n",
    "    transforms.Pad(padding=IMG_PADDING, fill=0), # pad to be square!\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(), # transform it into a torch tensor\n",
    "    # lambda x: x>0,\n",
    "    # lambda x: x.float(),\n",
    "    ])  \n",
    "\n",
    "params.img_binarize = IMG_BINZARIZE\n",
    "\n",
    "# fetch dataloaders\n",
    "new_test_dl = data_loader.fetch_dataloader('new_test', eval_transformer, data_dir, layer_name, label_split, params)\n",
    "\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'evaluate.log'))\n",
    "\n",
    "logging.info(\"Starting evaluation\")\n",
    "\n",
    "# Reload weights from the saved file\n",
    "# Name of the file in --model_dir containing weights to reload before evaluation\n",
    "restore_eval_file = 'last' # 'best' or 'last' or None\n",
    "map_location = None if params.cuda else 'cpu'\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_eval_file + '.pth.tar'), model, map_location=map_location)\n",
    "\n",
    "# Evaluate\n",
    "X_test, y_test_prob, y_test_pred, y_test, test_metrics = evaluate(model, loss_fn, new_test_dl, metrics, params)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_eval_file))\n",
    "utils.save_dict_to_json(test_metrics, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
