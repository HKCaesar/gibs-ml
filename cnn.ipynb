{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "import utils\n",
    "import models.net as net\n",
    "import models.data_loader as data_loader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (30.0, 16.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the .txt metadata about dataset\n",
    "data_dir = 'data/4326'\n",
    "layer_name = 'VIIRS_SNPP_CorrectedReflectance_TrueColor'\n",
    "label_split = os.path.join(data_dir, layer_name + \".txt\")\n",
    "\n",
    "# Directory containing params.json\n",
    "model_dir = 'experiments/base_model'\n",
    "\n",
    "# Name of the file in --model_dir containing weights to reload before training\n",
    "restore_file = 'best' # 'best' or 'train' or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "    \n",
    "    X = None\n",
    "    outputs_prob = None\n",
    "    outputs_pred = None\n",
    "    labels = None\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    with tqdm_notebook(total=len(dataloader)) as t:\n",
    "        for data_batch, labels_batch in dataloader:\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                data_batch, labels_batch = data_batch.cuda(async=True), labels_batch.cuda(async=True)\n",
    "            # fetch the next evaluation batch\n",
    "            data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output\n",
    "            output_batch = model(data_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            output_pred_batch = np.argmax(output_batch, axis=1)\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # save the outputs and labels\n",
    "            if X is None:\n",
    "                X = data_batch\n",
    "                outputs_prob = output_batch\n",
    "                outputs_pred = output_pred_batch\n",
    "                labels = labels_batch\n",
    "            else:\n",
    "                X = np.append(X, data_batch, axis=0)\n",
    "                outputs_prob = np.append(outputs_prob, output_batch, axis=0)\n",
    "                outputs_pred = np.append(outputs_pred, output_pred_batch, axis=0)\n",
    "                labels = np.append(labels, labels_batch, axis=0)\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data[0]\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Eval metrics : \" + metrics_string)\n",
    "    \n",
    "    return X, outputs_prob, outputs_pred, labels, metrics_mean\n",
    "\n",
    "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm_notebook(total=len(dataloader)) as t:\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(async=True), labels_batch.cuda(async=True)\n",
    "            # convert to torch Variables\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output and loss\n",
    "            output_batch = model(train_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                                 for metric in metrics}\n",
    "                summary_batch['loss'] = loss.data[0]\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.data[0])\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics, params, model_dir,\n",
    "                       restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(model_dir, restore_file + '.pth.tar')\n",
    "        logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "        utils.load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    best_val_score = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        _, _, _, _, val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)\n",
    "\n",
    "        val_score = val_metrics['F1']\n",
    "        is_best = val_score>best_val_score\n",
    "\n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            print(\"best_val_score {:05.3f}\".format(best_val_score))\n",
    "            print(\"val_score {:05.3f}\".format(val_score))\n",
    "            best_val_score = val_score\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "# Load the paramaters from file\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "params = utils.Params(json_path)\n",
    "\n",
    "# use GPU if available\n",
    "params.cuda = torch.cuda.is_available()\n",
    "print(\"GPU available: {}\".format(params.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "- done.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducible experiments\n",
    "torch.manual_seed(230)\n",
    "if params.cuda: torch.cuda.manual_seed(230)\n",
    "    \n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Loading the datasets...\")\n",
    "\n",
    "# fetch dataloaders\n",
    "dataloaders = data_loader.fetch_dataloader(['train', 'val'], data_dir, layer_name, label_split, params)\n",
    "train_dl = dataloaders['train']\n",
    "val_dl = dataloaders['val']\n",
    "\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and optimizer\n",
    "model = net.Net(params, num_classes=2)\n",
    "\n",
    "# Define the loss\n",
    "loss_fn = net.loss_fn\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "# fetch loss function and metrics\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "def recall(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the recall, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) recall in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return recall_score(labels, outputs)\n",
    "\n",
    "def precision(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the precision, given the outputs and labels for all images. \n",
    "    \n",
    "    The precision is the ratio tp / (tp + fp) where tp is the number \n",
    "    of true positives and fp the number of false positives. \n",
    "    \n",
    "    The precision is intuitively the ability of the classifier not to \n",
    "    label as positive a sample that is negative.\n",
    "    \n",
    "    The best value is 1 and the worst value is 0.\n",
    "    \n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) precision in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return precision_score(labels, outputs)\n",
    "\n",
    "def f1_metric(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the F1 score, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size x 2 - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1]\n",
    "\n",
    "    Returns: (float) F1 score in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return f1_score(labels, outputs)\n",
    "    \n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'F1': f1_metric, # weighted average of precision and recall\n",
    "    'recall': recall, \n",
    "    'precision': precision,\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting training for 100 epoch(s)\n",
      "Restoring parameters from experiments/base_model/best.pth.tar\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7069f0fd16854d8c9b72f062f9cff359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xue/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:112: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/xue/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "- Train metrics: F1: 0.875 ; recall: 0.778 ; precision: 1.000 ; accuracy: 0.969 ; loss: 0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97532309ca4c49a382de96a705912b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xue/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "- Eval metrics : F1: 0.635 ; recall: 0.467 ; precision: 1.000 ; accuracy: 0.948 ; loss: 0.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_val_score 0.000\n",
      "val_score 0.635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d061b88fa6ec4a27b39687b991bbd06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 1.000 ; recall: 1.000 ; precision: 1.000 ; accuracy: 1.000 ; loss: 0.063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820933ddbf2c4f758f6eaf459dbaca41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xue/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/xue/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "- Eval metrics : F1: 0.376 ; recall: 0.267 ; precision: 0.667 ; accuracy: 0.937 ; loss: 0.222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e201b26f1042dda237c3f7db010cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 0.857 ; recall: 0.750 ; precision: 1.000 ; accuracy: 0.969 ; loss: 0.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3333b76ffc504138aba667a349045844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.933 ; recall: 0.878 ; precision: 1.000 ; accuracy: 0.984 ; loss: 0.230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best accuracy\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_val_score 0.635\n",
      "val_score 0.933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83ede21987848c5bf674de451d743db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 0.952 ; recall: 0.909 ; precision: 1.000 ; accuracy: 0.984 ; loss: 0.082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d900150a3cb94a04bb481eca3f30b898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.896 ; recall: 0.822 ; precision: 1.000 ; accuracy: 0.979 ; loss: 0.198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a97af6c55354ddda8ae68fb664ee481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 0.909 ; recall: 0.833 ; precision: 1.000 ; accuracy: 0.984 ; loss: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b464715ff12d4a378eeb98421ffee4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.790 ; recall: 0.689 ; precision: 1.000 ; accuracy: 0.958 ; loss: 0.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d03b92f13c452faf6486d838b51aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 1.000 ; recall: 1.000 ; precision: 1.000 ; accuracy: 1.000 ; loss: 0.073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c007301125f348df93f3ca82dd680252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.852 ; recall: 0.767 ; precision: 1.000 ; accuracy: 0.974 ; loss: 0.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0437667456348f5bcc3629395ba110d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 1.000 ; recall: 1.000 ; precision: 1.000 ; accuracy: 1.000 ; loss: 0.073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcee07fdaea645478b056386e5402026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.746 ; recall: 0.633 ; precision: 1.000 ; accuracy: 0.953 ; loss: 0.142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c3ae1f751a4770a9478e6a0a9e972b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 1.000 ; recall: 1.000 ; precision: 1.000 ; accuracy: 1.000 ; loss: 0.078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cba07cc52d4f759007b6207f33f2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.421 ; recall: 0.322 ; precision: 0.667 ; accuracy: 0.942 ; loss: 0.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a168681132b4e16af8ab27256e740ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 1.000 ; recall: 1.000 ; precision: 1.000 ; accuracy: 1.000 ; loss: 0.068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8b1743766d40a7a9640057f7f12393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Eval metrics : F1: 0.421 ; recall: 0.322 ; precision: 0.667 ; accuracy: 0.942 ; loss: 0.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca21b04516245b283da52d17ab34ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Train metrics: F1: 1.000 ; recall: 1.000 ; precision: 1.000 ; accuracy: 1.000 ; loss: 0.061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f881cfaf73ba47688b78e3291b95f545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "train_and_evaluate(model, train_dl, val_dl, optimizer, loss_fn, metrics, params, model_dir, restore_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'evaluate.log'))\n",
    "\n",
    "# Create the input data pipeline\n",
    "logging.info(\"Creating the dataset...\")\n",
    "\n",
    "# fetch dataloaders\n",
    "dataloaders = data_loader.fetch_dataloader(['test'], data_dir, layer_name, label_split, params)\n",
    "test_dl = dataloaders['test']\n",
    "\n",
    "logging.info(\"- done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting evaluation\")\n",
    "\n",
    "# Reload weights from the saved file\n",
    "utils.load_checkpoint(os.path.join(model_dir, restore_file + '.pth.tar'), model)\n",
    "\n",
    "# Evaluate\n",
    "X_test, y_test_prob, y_test_pred, y_test, test_metrics = evaluate(model, loss_fn, test_dl, metrics, params)\n",
    "save_path = os.path.join(model_dir, \"metrics_test_{}.json\".format(restore_file))\n",
    "utils.save_dict_to_json(test_metrics, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(results, categories, normalizeMatrix = True, plotConfusion=False):\n",
    "    \"\"\" Generate confusion matrix\n",
    "    - results = list of tuples of (correct label, predicted label)\n",
    "    - categories = list of category names\n",
    "    Returns confusion matrix; rows are correct labels and columns are predictions\n",
    "    \"\"\"\n",
    "    # Empty confusion matrix\n",
    "    matrix = np.zeros((len(categories),len(categories)))\n",
    "\n",
    "    # Iterate over all labels and populate matrix\n",
    "    for label, pred in results:\n",
    "        matrix[label, pred] += 1\n",
    "\n",
    "    # Print matrix and percent accuracy\n",
    "    accuracy = float(np.trace(matrix)) * 100.0 / len(results)\n",
    "    print('Accuracy: ' +  str(accuracy) + '%')\n",
    "\n",
    "    # Normalize confusion matrix\n",
    "    if normalizeMatrix:\n",
    "      print(\"Non-normalized conf_matrix:\")\n",
    "      print(matrix)\n",
    "      s = np.sum(matrix,1) # Sum each row\n",
    "      for i in range(matrix.shape[0]):\n",
    "        # Normalization handles class imbalance in training set\n",
    "        matrix[i,:] /= s[i]\n",
    "\n",
    "    # Save matrix to file:\n",
    "    # np.save(\"confusion_matrix.npy\",matrix)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    if plotConfusion:\n",
    "        plot_confusion_matrix(matrix, categories)\n",
    "\n",
    "def plot_confusion_matrix(cm, names=None, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(4)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add labels to confusion matrix:\n",
    "    if names is None:\n",
    "        names = range(cm.shape[0])\n",
    "\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Correct label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "classes = ['normal', 'abnormal']\n",
    "confusion_matrix(list(zip(y_test_pred, y_test)), classes, plotConfusion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(X_test[0]).astype('uint8').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize misclassified examples from the validation dataset.\n",
    "num_classes = len(classes)\n",
    "max_examples = 5\n",
    "\n",
    "incorrect_idx = [idx for idx, incorrect in enumerate(np.array(y_test) != np.array(y_test_pred)) if incorrect]\n",
    "if len(incorrect_idx) > max_examples:\n",
    "    incorrect_idx = incorrect_idx[:max_examples]\n",
    "num_incorrect = len(incorrect_idx)\n",
    "\n",
    "for y, idx in enumerate(incorrect_idx):\n",
    "    for i in range(3):\n",
    "        plt_idx = i * num_incorrect + y + 1\n",
    "        if i == 0:\n",
    "            plt.subplot(3, num_incorrect, plt_idx)\n",
    "            plt.imshow(np.squeeze(X_test[idx]).astype('uint8'))\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Prediction: %s\\n Truth: %s\" % (classes[y_test_pred[idx]], classes[y_test[idx]]))\n",
    "        elif i == 1:\n",
    "            plt.subplot(3, num_incorrect, plt_idx)\n",
    "            log_probabilities = y_test_prob[idx]\n",
    "            probabilities = np.exp(log_probabilities)\n",
    "            y_pos = np.arange(len(classes))\n",
    "            plt.bar(y_pos, probabilities)\n",
    "            plt.xticks(y_pos, classes, rotation=45)\n",
    "            plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)\n",
    "img_size = (16, 8)\n",
    "num_channels = 1\n",
    "\n",
    "# Visualize the learned weights for each class.\n",
    "# Depending on your choice of learning rate and regularization strength, these may\n",
    "# or may not be nice to look at.\n",
    "w = model.fc2.weight.data.numpy() # convert to numpy arrays\n",
    "print(w.shape)\n",
    "w = w.reshape(num_classes, img_size[1], img_size[0], num_channels)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "      \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[i, :, :, :].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
